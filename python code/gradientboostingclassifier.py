# -*- coding: utf-8 -*-
"""GradientBoostingClassifier.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-1He9JLJXBQ712T-AGL4wrfz7gbB1BBb

***GRADIENT BOOSTING CLASSIFIER***

---



---
"""

import pandas as pd
import numpy as np

data=pd.read_csv("/content/pd_speech_features.csv")
data.head()

"""**Preprocessing of dataset**


---





"""

data.shape

data.describe().transpose()

data=data.drop(['id'],axis=1)
data.info()

data=data.drop_duplicates()
data=data.dropna()
data.info()

y=data.loc[:,'class']
x=data.iloc[:,:-1]

x.head()

y.head()

"""**Check for unbalanced dataset**

---


"""

import seaborn as sns
import matplotlib.pyplot as plt
sns.set_style('whitegrid')
sns.set_context('paper')
sns.set_palette('GnBu_d')
a = sns.catplot(x='class', data=data, kind='count')
a.fig.suptitle('Number of Samples in Each Class', y=1.03)
a.set(ylabel='Number of Samples', xlabel='Have Parkinson')
plt.show()

""" **Univariate Analysis**

---


"""

# Commented out IPython magic to ensure Python compatibility.
import seaborn as sns
import matplotlib.pyplot as plt
# %matplotlib inline
fig, ax = plt.subplots(1,3,figsize=(16,10)) 
sns.boxplot(x='f1',data=data, ax=ax[0],orient='v') 
sns.boxplot(x='f2',data=data, ax=ax[1],orient='v')
sns.boxplot(x='f3',data=data, ax=ax[1],orient='v')
sns.boxplot(x='f4',data=data, ax=ax[1],orient='v')
sns.boxplot(x='PPE',data=data,ax=ax[2],orient='v')

"""The above figure shows the box plot of the frequency variation. All the three variations have outliers. Generally speaking, decision trees are able to handle outliers. It is very unlikely that decision tree will create a leaf to isolate them"""

fig, ax = plt.subplots(1,3,figsize=(16,8)) 
sns.distplot(data['mean_MFCC_2nd_coef'],ax=ax[0]) 
sns.distplot(data[  'IMF_SNR_SEO'],ax=ax[1]) 
sns.distplot(data[  'GNE_NSR_SEO'],ax=ax[2])

"""The measures of vocal fundamental frequency are shown above"""

fig, ax = plt.subplots(2,3,figsize=(16,8)) 
sns.distplot(data['locShimmer'],ax=ax[0,0]) 
sns.distplot(data['apq11Shimmer'],ax=ax[0,1]) 
sns.distplot(data['locDbShimmer'],ax=ax[0,2]) 
sns.distplot(data['apq3Shimmer'],ax=ax[1,0]) 
sns.distplot(data['apq5Shimmer'],ax=ax[1,1]) 
sns.distplot(data['ddaShimmer'],ax=ax[1,2])

"""For all of the above graphs, we can observe that the measure of variation in amplitude is positively skewed"""

fig, ax = plt.subplots(1,2,figsize=(16,8))
sns.boxplot(x='class',y='DFA',data=data,ax=ax[0])
sns.boxplot(x='class',y='DFA',data=data,ax=ax[1])

# For categorical predictors
cols = ["locPctJitter","locAbsJitter","rapJitter","ppq5Jitter","ddpJitter"]
fig, axs = plt.subplots(ncols = 5,figsize=(16,8))
fig.tight_layout()
for i in range(0,len(cols)):
    sns.boxplot(x='class',y=cols[i],data=data, ax = axs[i])

"""People who are suffering for PD tend to have higher jitter %.  The variation of fundamental frequency is in a low range for people who is normal.

**Applying Model** 

---
"""

from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test= train_test_split(x,y,test_size=0.8,random_state=23)

from sklearn.ensemble import GradientBoostingClassifier
gbcl_model = GradientBoostingClassifier(n_estimators = 50, learning_rate = 0.05)
gbcl_model = gbcl_model.fit(X_train,y_train)

"""**Accuracy**

---


"""

y_pred = gbcl_model.predict(X_test)
print("Test Accuracy:",gbcl_model.score(X_test , y_test))

"""**Evaluation Metrics**

---


"""

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
cm = confusion_matrix(y_test, y_pred)
cm

disp = ConfusionMatrixDisplay(confusion_matrix=cm) 
disp.plot()

from sklearn.metrics import precision_recall_curve
precision, recall, thresholds = precision_recall_curve(y_pred,y_test) 
print("Precision: ",precision) 
print("Recall: ",recall) 
print("Thresholds: ",thresholds)

import matplotlib.pyplot as plt
fig, ax = plt.subplots()
ax.plot(recall, precision, color='purple')
#add axis labels to plot 
ax.set_title('Precision-Recall Curve') 
ax.set_ylabel('Precision')
ax.set_xlabel('Recall')
#display plot 
plt.show()

from sklearn.metrics import roc_auc_score 
print("ROC Accuracy score is: ",roc_auc_score(y,gbcl_model.predict_proba(x)[:, 1])*100,"%")

from sklearn import metrics as m
m.plot_roc_curve(gbcl_model, X_test, y_test)

from sklearn.model_selection import learning_curve
train_sizes, train_scores, test_scores, fit_times, _ = learning_curve(gbcl_model, x, y, cv=30,return_times=True)
plt.plot(train_sizes,np.mean(train_scores,axis=1))