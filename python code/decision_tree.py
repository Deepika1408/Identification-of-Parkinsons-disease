# -*- coding: utf-8 -*-
"""Decision Tree

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ZLKo-zIVva1Bxq1kwLcdIe4rG-opQeBf

***Decision Tree***

---



---
"""

import pandas as pd
import numpy as np
import matplotlib as plt

data=pd.read_csv("/content/pd_speech_features.csv")
data.head()

"""**Preprocessing of dataset**

---


"""

data=data.drop_duplicates()
data=data.dropna()
data.info()

data=data.drop(['id'],axis=1)
data.info()

data.shape

data.columns

data.describe().transpose()

y=data.loc[:,'class']
x=data.iloc[:,:-1]

y.head()

x.head()

"""**Check for unbalanced dataset**

---


"""

import seaborn as sns
import matplotlib.pyplot as plt
sns.set_style('whitegrid')
sns.set_context('paper')
sns.set_palette('GnBu_d')
a = sns.catplot(x='class', data=data, kind='count')
a.fig.suptitle('Number of Samples in Each Class', y=1.03)
a.set(ylabel='Number of Samples', xlabel='Have Parkinson')
plt.show()

"""**Univariate Analysis**

---


"""

# Commented out IPython magic to ensure Python compatibility.
import seaborn as sns
import matplotlib.pyplot as plt
# %matplotlib inline
fig, ax = plt.subplots(1,3,figsize=(16,10))
sns.boxplot(x='f1',data=data, ax=ax[0],orient='v')
sns.boxplot(x='f2',data=data, ax=ax[1],orient='v')
sns.boxplot(x='f3',data=data, ax=ax[1],orient='v')
sns.boxplot(x='f4',data=data, ax=ax[1],orient='v')
sns.boxplot(x='PPE',data=data,ax=ax[2],orient='v')

"""The above figure shows the box plot of the frequency variation. All the three variations have outliers. Generally speaking, decision trees are able to handle outliers. It is very unlikely that decision tree will create a leaf to isolate them"""

fig, ax = plt.subplots(1,3,figsize=(16,8))
sns.distplot(data['mean_MFCC_2nd_coef'],ax=ax[0])
sns.distplot(data[ 'IMF_SNR_SEO'],ax=ax[1])
sns.distplot(data[ 'GNE_NSR_SEO'],ax=ax[2])

"""The measures of vocal fundamental frequency are shown above"""

fig, ax = plt.subplots(2,3,figsize=(16,8))
sns.distplot(data['locShimmer'],ax=ax[0,0])
sns.distplot(data['apq11Shimmer'],ax=ax[0,1])
sns.distplot(data['locDbShimmer'],ax=ax[0,2])
sns.distplot(data['apq3Shimmer'],ax=ax[1,0])
sns.distplot(data['apq5Shimmer'],ax=ax[1,1])
sns.distplot(data['ddaShimmer'],ax=ax[1,2])

"""For all of the above graphs, we can observe that the measure of variation in amplitude is positively skewed"""

fig, ax = plt.subplots(1,2,figsize=(16,8))
sns.boxplot(x='class',y='DFA',data=data,ax=ax[0])
sns.boxplot(x='class',y='DFA',data=data,ax=ax[1])

cols = ["locPctJitter","locAbsJitter","rapJitter","ppq5Jitter","ddpJitter"]
fig, axs = plt.subplots(ncols = 5,figsize=(16,8))
fig.tight_layout()
for i in range(0,len(cols)):
  sns.boxplot(x='class',y=cols[i],data=data, ax = axs[i])

"""People who are suffering for PD tend to have higher jitter %.  The variation of fundamental frequency is in a low range for people who is normal.

**Applying Model**

---
"""

from sklearn.model_selection import train_test_split
x_train,x_test,y_train,y_test= train_test_split(x,y,test_size=0.2)

from sklearn.tree import DecisionTreeClassifier
clf=DecisionTreeClassifier(max_depth=5)
clf.fit(x_train,y_train)

"""**Accuracy**

---


"""

y_pred=clf.predict(x_test)

from sklearn.metrics import accuracy_score
print("Test Accuracy of Decision tree:",accuracy_score(y_pred,y_test)*100)

"""**Decision Tree**

---


"""

from sklearn.tree import plot_tree
import matplotlib.pyplot as pltt
pltt.figure(figsize=(14,14))
plot_tree(clf,fontsize=10,filled=True)
pltt.title("Decison Tree")
pltt.show()

"""
**Evaluation Metrics**

---


"""

from sklearn.metrics import precision_recall_curve
precision, recall, thresholds = precision_recall_curve(y_pred,y_test)
print("Precision: ",precision)
print("Recall: ",recall)
print("Thresholds: ",thresholds)

import matplotlib.pyplot as plt
fig, ax = plt.subplots()
ax.plot(recall, precision, color='purple')
#add axis labels to plot
ax.set_title('Precision-Recall Curve')
ax.set_ylabel('Precision')
ax.set_xlabel('Recall')
#display plot
plt.show()

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
cm = confusion_matrix(y_test, y_pred)
cm

disp = ConfusionMatrixDisplay(confusion_matrix=cm)
disp.plot()

from sklearn.metrics import roc_auc_score 
print("ROC Accuracy score is: ",roc_auc_score(y,clf.predict_proba(x)[:, 1])*100,"%")

from sklearn.model_selection import learning_curve
train_sizes, train_scores, test_scores, fit_times, _ = learning_curve(clf, x, y, cv=30,return_times=True)
plt.plot(train_sizes,np.mean(train_scores,axis=1))

