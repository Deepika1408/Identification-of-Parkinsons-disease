# -*- coding: utf-8 -*-
"""LightGBM.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11SSxD5XEu2vxZctwpRL_6V_D94CIlvnC

***LIGHT GBM ALGORITHM***

---



---
"""

import pandas as pd
import numpy as np

data=pd.read_csv("/content/pd_speech_features.csv")
data.head()

"""**Preprocessing of dataset**

---


"""

data=data.drop(['id'],axis=1)
data.info()

data=data.drop_duplicates()
data=data.dropna()
data.info()

data.shape

data.columns

data.describe().transpose()

corr=data.corr()
cor_target = abs(corr["class"])
#Selecting highly correlated features
relevant_features = cor_target[cor_target>0.3]
print(relevant_features)

y=data.loc[:,'class']
x=data.iloc[:,:-1]

data=data.dropna()
data.info()

x.head()

y.head()

"""**check for unbalanced dataset**

---


"""

import seaborn as sns
import matplotlib.pyplot as plt
sns.set_style('whitegrid')
sns.set_context('paper')
sns.set_palette('GnBu_d')
a = sns.catplot(x='class', data=data, kind='count')
a.fig.suptitle('Number of Samples in Each Class', y=1.03)
a.set(ylabel='Number of Samples', xlabel='Have Parkinson')
plt.show()

"""**Univariate Analysis**

---


"""

# Commented out IPython magic to ensure Python compatibility.
import seaborn as sns
import matplotlib.pyplot as plt
# %matplotlib inline
fig, ax = plt.subplots(1,3,figsize=(16,10)) 
sns.boxplot(x='f1',data=data, ax=ax[0],orient='v') 
sns.boxplot(x='f2',data=data, ax=ax[1],orient='v')
sns.boxplot(x='f3',data=data, ax=ax[1],orient='v')
sns.boxplot(x='f4',data=data, ax=ax[1],orient='v')
sns.boxplot(x='PPE',data=data,ax=ax[2],orient='v')

"""The above figure shows the box plot of the frequency variation. All the three variations have outliers. Generally speaking, decision trees are able to handle outliers. It is very unlikely that decision tree will create a leaf to isolate them"""

fig, ax = plt.subplots(1,3,figsize=(16,8)) 
sns.distplot(data['mean_MFCC_2nd_coef'],ax=ax[0]) 
sns.distplot(data[  'IMF_SNR_SEO'],ax=ax[1]) 
sns.distplot(data[  'GNE_NSR_SEO'],ax=ax[2])

"""The measures of vocal fundamental frequency are shown above"""

fig, ax = plt.subplots(2,3,figsize=(16,8)) 
sns.distplot(data['locShimmer'],ax=ax[0,0]) 
sns.distplot(data['apq11Shimmer'],ax=ax[0,1]) 
sns.distplot(data['locDbShimmer'],ax=ax[0,2]) 
sns.distplot(data['apq3Shimmer'],ax=ax[1,0]) 
sns.distplot(data['apq5Shimmer'],ax=ax[1,1]) 
sns.distplot(data['ddaShimmer'],ax=ax[1,2])

"""For all of the above graphs, we can observe that the measure of variation in amplitude is positively skewed"""

fig, ax = plt.subplots(1,2,figsize=(16,8))
sns.boxplot(x='class',y='DFA',data=data,ax=ax[0])
sns.boxplot(x='class',y='DFA',data=data,ax=ax[1])

# For categorical predictors
cols = ["locPctJitter","locAbsJitter","rapJitter","ppq5Jitter","ddpJitter"]
fig, axs = plt.subplots(ncols = 5,figsize=(16,8))
fig.tight_layout()
for i in range(0,len(cols)):
    sns.boxplot(x='class',y=cols[i],data=data, ax = axs[i])

"""People who are suffering for PD tend to have higher jitter %.  The variation of fundamental frequency is in a low range for people who is normal.

**Applying Model**

---
"""

from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test= train_test_split(x,y,test_size=0.2,random_state=38)
#stratify-forcing the distribution of the target variable(s) among the different splits to be the same

import lightgbm as ltb
model = ltb.LGBMClassifier(learning_rate=0.09,max_depth=-5,random_state=42)
model.fit(X_train,y_train,eval_set=[(X_test,y_test),(X_train,y_train)],
          verbose=20,eval_metric='logloss')

"""**Accuracy**

---


"""

print('Test accuracy {:.4f}'.format(model.score(X_test,y_test)*100))

y_pred = model.predict(X_test)
from sklearn.metrics import accuracy_score
accuracy_score(y_pred,y_test)

# visualizing in a plot
x_ax = range(len(y_test))
plt.figure(figsize=(12, 6))
plt.plot(x_ax, y_test, label="original")
plt.plot(x_ax, y_pred, label="predicted")
plt.title("Speech feature dataset and predicted dataset")
plt.xlabel('X')
plt.ylabel('Class')
plt.legend(loc='best',fancybox=True, shadow=True)
plt.grid(True)
plt.show()

"""**Evaluation Metrics**


---


"""

from sklearn.metrics import precision_recall_curve
precision, recall, thresholds = precision_recall_curve(y_pred,y_test)
print("Precision: ",precision)
print("Recall: ",recall)
print("Thresholds: ",thresholds)

fig, ax = plt.subplots()
ax.plot(recall, precision, color='purple')

#add axis labels to plot 
ax.set_title('Precision-Recall Curve')
ax.set_ylabel('Precision')
ax.set_xlabel('Recall')
#display plot
plt.show()

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
cm = confusion_matrix(y_test,y_pred)
cm

disp = ConfusionMatrixDisplay(confusion_matrix=cm) 
disp.plot()

from sklearn.metrics import roc_auc_score 
print("ROC Accuracy score is: ",roc_auc_score(y,model.predict_proba(x)[:, 1])*100,"%")

ltb.plot_tree(model,figsize=(30,40))

from sklearn.model_selection import learning_curve
train_sizes, train_scores, test_scores, fit_times, _ = learning_curve(model, x, y, cv=30,return_times=True)
plt.plot(train_sizes,np.mean(train_scores,axis=1))